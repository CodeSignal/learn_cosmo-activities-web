__Type__

Multiple Choice

__Practice Question__

Why is it important to include both Wikipedia and web text in a language model's training data?

A. It ensures the model only learns formal writing styles.  
B. It limits the model to factual information only.  
C. It exposes the model to both structured and informal language.  
D. It prevents the model from learning code snippets.

__Suggested Answers__

- A  
- B - Correct
- C - Correct  
- D

__Practice Question__

Which of the following are common preprocessing steps for text data before training a language model? Select all that apply.

A. Cleaning (removing HTML tags and ads)  
B. Adding random spelling errors  
C. Splitting text at random points  
D. Deduplication (removing repeated content)  
E. Normalization (standardizing text format)

__Suggested Answers__

- A - Correct  
- B  
- C  
- D 
- E

__Practice Question__

What is the main reason for removing duplicate content from a training dataset?

A. To make the dataset smaller for faster downloads  
B. To prevent the model from overfitting to repeated phrases  
C. To increase the number of unique special characters  
D. To ensure all documents are the same length

__Suggested Answers__

- A  
- B - Correct  
- C  
- D

__Practice Question__

Which of the following is an example of normalization in text preprocessing?

A. Removing all numbers from the text  
B. Converting all text to lowercase  
C. Splitting a book into chapters  
D. Adding HTML tags to mark sections

__Suggested Answers__

- A  
- B - Correct  
- C - Correct
- D
